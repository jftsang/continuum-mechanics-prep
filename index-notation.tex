\documentclass{article}
\input{preamble-light.tex}
\title{Index notation}
\author{T. J. Crawford, J. Goedecke, P. Haas, E. Lauga, J. Munro, J. M. F. Tsang}

\begin{document}
\maketitle

The relevant Cambridge undergraduate course is IA Vectors and Matrices.

Index notation and the summation convention are very useful shorthands for writing otherwise long vector equations. Whenever a quantity is summed over an index which appears exactly twice in each term in the sum, we leave out the summation sign.

Simple example: The vector $\bs{x} = (x_1,x_2,x_3)$ can be written as 
$$ \bs{x} = x_1 \bs{e}_1 + x_2 \bs{e}_2 + x_3 \bs{e}_3 = \sum_{i=1}^3 x_i\bs{e}_i. $$
Under the summation convention, we simply write this as
$$ \bs{x} = x_i \bs{e}_i. $$

Most vector, matrix and tensor expressions that occur in practice can be written
very succinctly using this notation:
\begin{itemize}
    \item Dot products: $ \bs{u}\cdot\bs{v} = u_iv_i $
    \item Cross products: $ (\bs{u}\cross\bs{v})_i = \epsilon_{ijk}u_jv_k $ (see below) 
    \item Matrix multiplication: $(\bs{A}\cdot\bs{v})_i = A_{ij}v_j$
    \item Trace of a matrix: $\mathrm{tr}(\bs{A}) = A_{ii}$
    \item Tensor contraction: $ \Delta = 2\mu\bs{e}:\bs{e} = 2\mu e_{ij}e_{ij} $
    \item Divergence: $\divg\bs{u} = \dpd{u_i}{x_i} $
    \item Laplacian: $\grad^2 f = \dpd[2]{f}{x_i} $ (but beware---the vector
    Laplacian $\grad^2\bs{A} = \grad(\divg\bs{A}) - \curl(\curl\bs{A})$ is given
    by $\dpd[2]{A_j}{x_i}$ only when we work in Cartesian coordinates)
\end{itemize}

An index that appears exactly twice in a term is implicitly summed over; such an index is called a \textit{dummy index}. The letter used for a dummy index is not important. An index that appears only once is called a \textit{free index}. 

No index may appear three times or more in an expression. For example, the expression $u_iv_iw_i$ is illegal in summation convention. If you do need the expression $\sum_{i=1}^3 u_iv_iw_i$, write out the summation sign. Likewise, you can write `no sum' after an expression such as $u_i v_i$ to indicate that $i$ should be understood as a free index. These situations are fairly rare.

There are two special symbols in summation convention. The \textit{Kronecker delta} $\delta_{ij}$ is defined by
\begin{equation}
 \delta_{ij} = \begin{cases}
  1 & i = j \\
  0 & i\neq j.
 \end{cases}
\end{equation}
The identity matrix is therefore $(\bs{I})_{ij} = \delta_{ij}$. The \textit{Levi-Civita symbol} $\epsilon_{ijk}$ is defined by 
\begin{align}
    \epsilon_{123} = \epsilon_{231} = \epsilon_{312} &= +1 \\
    \epsilon_{132} = \epsilon_{213} = \epsilon_{321} &= -1 \\
    \epsilon_{ijk} = 0 \text{ otherwise.}
\end{align}
That is, $\epsilon_{ijk}$ is given by the parity of the permutation $(1,2,3)\rightarrow(i,j,k)$. 

It can be shown that 
\begin{equation} \label{eqn:eedd}
 \epsilon_{ijk}\epsilon{ilm} = \delta_{jl}\delta_{km} - \delta_{jm}\delta_{kl}. 
\end{equation}
The proof is long and tedious, but simply involves writing out all the terms and collecting them together carefully. 

\section*{Exercises}

Show that the above shorthands do give the expressions that they claim to.

What is the norm-squared of a vector, $|\bs{u}|^2$, in index notation?

What is the curl of a vector field, $\curl\bs{F}$, in index notation?

Show that the divergence theorem can be written as
$$ \iint_{\partial V} F_j n_j \dif S = \iiint_V \dpd{F_j}{x_j} \dif V. $$
How can Stokes' theorem be written?

Use the identity \ref{eqn:eedd} to show that
$$ \bs{a}\cross(\bs{b}\cross\bs{c}) = (\bs{a}\cdot\bs{c})\bs{b} - (\bs{a}\cdot\bs{b})\bs{c}. $$

Show that the advective derivative can be written as 
$$ (\bs{u}\cdot\grad)\bs{u} = \grad\left(\frac{1}{2}\bs{u}^2\right) - \bs{u}\cross(\curl\bs{u}). $$

\end{document}
